# -*- coding: utf-8 -*-
"""ICLR2023_Data_collection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oSwhH15Y_W8VMG1ZxqmRfASxQOqsQgVU

# Imports
"""

# google colab installs

!pip install condacolab &> /dev/null
import condacolab
condacolab.install()

# install all packages in one call (+ use mamba instead of conda)
!mamba install xarray-datatree intake-esm gcsfs xmip aiohttp cartopy nc-time-axis cf_xarray xarrayutils "esmf<=8.3.1" xesmf &> /dev/null

# installations ( uncomment and run this cell ONLY when using google colab or kaggle )

# # properly install cartopy in colab to avoid session crash
!apt-get install libproj-dev proj-data proj-bin --quiet
!apt-get install libgeos-dev --quiet
!pip install cython --quiet
!pip install cartopy --quiet
!pip install geoviews

!apt-get -qq install python-cartopy python3-cartopy  --quiet
!pip uninstall -y shapely  --quiet
!pip install shapely --no-binary shapely  --quiet

!pip install boto3 --quiet

# you may need to restart the runtime after running this cell and that is ok

!pip install pydantic==2.3

# imports

import time

tic = time.time()

import intake
import numpy as np
import matplotlib.pyplot as plt
import xarray as xr
import xesmf as xe
from xmip.preprocessing import combined_preprocessing
from xarrayutils.plotting import shaded_line_plot
from datatree import DataTree
from xmip.postprocessing import _parse_metric
import cartopy.crs as ccrs
import pooch
import os
import tempfile
from scipy import stats

# Commented out IPython magic to ensure Python compatibility.
# functions

# %matplotlib inline

col = intake.open_esm_datastore(
    "https://storage.googleapis.com/cmip6/pangeo-cmip6.json"
)  # open an intake catalog containing the Pangeo CMIP cloud data


def load_cmip6(source_id, variable_id, member_id, table_id):  # load selected model
    cat = col.search(
        source_id=source_ids,
        variable_id=variable_id,
        member_id=member_id,
        table_id=table_id,
        grid_label="gn",
        experiment_id=[
            "historical",
            "ssp126",
            "ssp245",
            "ssp585",
        ],  # downloading the scenarios out of the total 5+historical
        require_all_on=["source_id"],
    )

    kwargs = dict(
        preprocess=combined_preprocessing,
        xarray_open_kwargs=dict(use_cftime=True),
        storage_options={"token": "anon"},
    )
    cat.esmcat.aggregation_control.groupby_attrs = ["source_id", "experiment_id"]
    dt = cat.to_datatree(**kwargs)

    return dt

import pydantic
pydantic.__version__

# helper functions

def pooch_load(filelocation=None,filename=None,processor=None):
    shared_location='/home/jovyan/shared/Data/Projects/ENSO' # this is different for each day
    user_temp_cache=tempfile.gettempdir()

    if os.path.exists(os.path.join(shared_location,filename)):
        file = os.path.join(shared_location,filename)
    else:
        file = pooch.retrieve(filelocation,known_hash=None,fname=os.path.join(user_temp_cache,filename),processor=processor)

    return file

"""# NOAA Data
- SST
- Precipitation
- Anomalies
"""

# Ocean surface temprature
filename_SST='sst.mnmean.nc'
url_SST = 'https://downloads.psl.noaa.gov/Datasets/noaa.ersst.v5/sst.mnmean.nc'

do_sst = xr.open_dataset(pooch_load(url_SST,filename_SST), drop_variables=['time_bnds'])

# Precipitation rate (notice the units in the plot below)
filename_prec_rate='precip.mon.mean.nc'
url_prec_rate='https://downloads.psl.noaa.gov/Datasets/cmap/enh/precip.mon.mean.nc'
do_pr = xr.open_dataset(pooch_load(url_prec_rate,filename_prec_rate))

# Air Temperature Anomalies
filename_tas='air.2x2.1200.mon.anom.comb.nc'
url_tas='https://downloads.psl.noaa.gov/Datasets/gistemp/combined/1200km/air.2x2.1200.mon.anom.comb.nc'
do_tas = xr.open_dataset(pooch_load(url_tas,filename_tas))

## Coordinates Indonesia

x_ind = slice(95, 141) # longitude
y_ind = slice(7,-12) # latitude

## Precipitation data for Indonesia
do_pr.precip

print(do_pr.precip)

print(do_pr.precip.sel(time=slice('1979-01-01','2023-07-01'), lat=y_ind, lon=x_ind))

rain = do_pr.precip.sel(time=slice('1979-01-01','2023-07-01'), lat=y_ind, lon=x_ind)

rain.shape

## Data time span available
12*(2023-1979)+7

## SST data for Indonesia
do_sst

print(do_sst.sst)

print(do_sst.sst.sel(time=slice('1979-01-01','2023-07-01'), lat=y_ind, lon=x_ind))

sst = do_sst.sst.sel(time=slice('1979-01-01','2023-07-01'), lat=y_ind, lon=x_ind)

## Air temperature anomalies data for Indonesia
do_tas

print(do_tas.air)

print(do_tas.air.sel(time=slice('1979-01-01','2023-07-01'), lat=y_ind, lon=x_ind))

temp = do_tas.air.sel(time=slice('1979-01-01','2023-07-01'), lat=y_ind, lon=x_ind)

"""### Plots"""

## Plot data
plt.title('Air Temperature anomaly')
temp.sel(time="1979-07-01").plot()
plt.show()

plt.title('Air Temperature anomaly')
temp.sel(time="2001-07-01").plot()
plt.show()

plt.title('Air Temperature anomaly')
temp.sel(time="2023-07-01").plot()

## Plot data
plt.title('SST Temperature')
sst.sel(time="1979-07-01").plot()
plt.show()

plt.title('SST Temperature')
sst.sel(time="2001-07-01").plot()
plt.show()

plt.title('SST Temperature')
sst.sel(time="2023-07-01").plot()

## Plot data
plt.title('Precipitation')
rain.sel(time="1979-07-01").plot()
plt.show()

plt.title('Precipitation')
rain.sel(time="2001-07-01").plot()
plt.show()

plt.title('Precipitation')
rain.sel(time="2023-07-01").plot()

"""Maps plots"""

## Air temperature anomalies

air_ind_last = do_tas.air.sel(time=slice('1979-01-01','2023-07-01'), lat=y_ind, lon=x_ind)[-1,:,:].squeeze()
air_ind_mid = do_tas.air.sel(time=slice('2021-07-01','2021-08-01'), lat=y_ind, lon=x_ind)[0,:,:].squeeze()
air_ind_firs = do_tas.air.sel(time=slice('1979-01-01','2023-07-01'), lat=y_ind, lon=x_ind)[6,:,:].squeeze()

fig, ax = plt.subplots(
    ncols=1, nrows=3, figsize=[15, 10], subplot_kw={"projection": ccrs.PlateCarree()}
)
# plot the model data
air_ind_firs.plot(
    ax=ax[0],
    x="lon",
    y="lat",
    vmin=air_ind_firs.min().compute(),
    vmax=air_ind_firs.max().compute(),
    cmap="viridis",
    robust=True # This deletes outliers: use 2nd and 98th percentiles of data
)

air_ind_mid.plot(
    ax=ax[1],
    x="lon",
    y="lat",
    vmin=air_ind_mid.min().compute(),
    vmax=air_ind_mid.max().compute(),
    cmap="viridis",
    robust=True # This deletes outliers: use 2nd and 98th percentiles of data
)

air_ind_last.plot(
    ax=ax[2],
    x="lon",
    y="lat",
    vmin=air_ind_last.min().compute(),
    vmax=air_ind_last.max().compute(),
    cmap="viridis",
    robust=True
)
ax[0].coastlines()
#ax[0].set_title(f"Indonesia Tree Cover - {np.datetime_as_string(ts_ind_first['time'].values, unit='D')}")

ax[1].coastlines()
#ax[1].set_title(f"Indonesia Tree Cover - {np.datetime_as_string(ts_ind_last['time'].values, unit='D')}")

ax[2].coastlines()
#ax[2].set_title(f"Indonesia Tree Cover - {np.datetime_as_string(ts_ind_last['time'].values, unit='D')}")

## SST temperature

sst_ind_last = do_sst.sst.sel(time=slice('1979-01-01','2023-07-01'), lat=y_ind, lon=x_ind)[-1,:,:].squeeze()
sst_ind_mid = do_sst.sst.sel(time=slice('2021-07-01','2021-08-01'), lat=y_ind, lon=x_ind)[0,:,:].squeeze()
sst_ind_firs = do_sst.sst.sel(time=slice('1979-01-01','2023-07-01'), lat=y_ind, lon=x_ind)[6,:,:].squeeze()

fig, ax = plt.subplots(
    ncols=1, nrows=3, figsize=[15, 10], subplot_kw={"projection": ccrs.PlateCarree()}
)
# plot the model data
sst_ind_firs.plot(
    ax=ax[0],
    x="lon",
    y="lat",
    vmin=sst_ind_firs.min().compute(),
    vmax=sst_ind_firs.max().compute(),
    cmap="viridis",
    robust=True # This deletes outliers: use 2nd and 98th percentiles of data
)

sst_ind_mid.plot(
    ax=ax[1],
    x="lon",
    y="lat",
    vmin=sst_ind_mid.min().compute(),
    vmax=sst_ind_mid.max().compute(),
    cmap="viridis",
    robust=True # This deletes outliers: use 2nd and 98th percentiles of data
)

sst_ind_last.plot(
    ax=ax[2],
    x="lon",
    y="lat",
    vmin=sst_ind_last.min().compute(),
    vmax=sst_ind_last.max().compute(),
    cmap="viridis",
    robust=True
)
ax[0].coastlines()
#ax[0].set_title(f"Indonesia Tree Cover - {np.datetime_as_string(ts_ind_first['time'].values, unit='D')}")

ax[1].coastlines()
#ax[1].set_title(f"Indonesia Tree Cover - {np.datetime_as_string(ts_ind_last['time'].values, unit='D')}")

ax[2].coastlines()
#ax[2].set_title(f"Indonesia Tree Cover - {np.datetime_as_string(ts_ind_last['time'].values, unit='D')}")

## precipitation

pr_ind_last = do_pr.precip.sel(time=slice('1979-01-01','2023-07-01'), lat=y_ind, lon=x_ind)[-1,:,:].squeeze()
pr_ind_mid = do_pr.precip.sel(time=slice('2021-07-01','2021-08-01'), lat=y_ind, lon=x_ind)[0,:,:].squeeze()
pr_ind_firs = do_pr.precip.sel(time=slice('1979-01-01','2023-07-01'), lat=y_ind, lon=x_ind)[6,:,:].squeeze()

fig, ax = plt.subplots(
    ncols=1, nrows=3, figsize=[15, 10], subplot_kw={"projection": ccrs.PlateCarree()}
)
# plot the model data
pr_ind_firs.plot(
    ax=ax[0],
    x="lon",
    y="lat",
    vmin=pr_ind_firs.min().compute(),
    vmax=pr_ind_firs.max().compute(),
    cmap="viridis",
    robust=True # This deletes outliers: use 2nd and 98th percentiles of data
)

pr_ind_mid.plot(
    ax=ax[1],
    x="lon",
    y="lat",
    vmin=pr_ind_mid.min().compute(),
    vmax=pr_ind_mid.max().compute(),
    cmap="viridis",
    robust=True # This deletes outliers: use 2nd and 98th percentiles of data
)

pr_ind_last.plot(
    ax=ax[2],
    x="lon",
    y="lat",
    vmin=pr_ind_last.min().compute(),
    vmax=pr_ind_last.max().compute(),
    cmap="viridis",
    robust=True
)
ax[0].coastlines()
#ax[0].set_title(f"Indonesia Tree Cover - {np.datetime_as_string(ts_ind_first['time'].values, unit='D')}")

ax[1].coastlines()
#ax[1].set_title(f"Indonesia Tree Cover - {np.datetime_as_string(ts_ind_last['time'].values, unit='D')}")

ax[2].coastlines()
#ax[2].set_title(f"Indonesia Tree Cover - {np.datetime_as_string(ts_ind_last['time'].values, unit='D')}")



"""#### Correlation"""

## precipitation

rain_timeseries = do_pr.sel(time=slice('1979-01-01','2023-07-01'), lat=y_ind, lon=x_ind)
rain_timeseries = rain_timeseries.mean(["lon", "lat"], keep_attrs=True)

rain_timeseries

#property_timeseries_mean = property_timeseries.groupby("time.year.month")
#property_timeseries_mean = property_timeseries.mean(dim=time)
rain_timeseries_mean = np.array(rain_timeseries.precip)
#print(property_timeseries.precip)

len(rain_timeseries_mean)

## Air temperature anomalies

tas_timeseries = do_tas.sel(time=slice('1979-01-01','2023-07-01'), lat=y_ind, lon=x_ind)
tas_timeseries = tas_timeseries.mean(["lon", "lat"], keep_attrs=True)
tas_timeseries_mean = np.array(tas_timeseries.air)

len(tas_timeseries_mean)

## SST

sst_timeseries = do_sst.sel(time=slice('1979-01-01','2023-07-01'), lat=y_ind, lon=x_ind)
sst_timeseries = sst_timeseries.mean(["lon", "lat"], keep_attrs=True)
sst_timeseries_mean = np.array(sst_timeseries.sst)

len(sst_timeseries_mean)

np.isnan(tas_timeseries_mean).sum()

np.isnan(sst_timeseries_mean).sum()

np.isnan(rain_timeseries_mean).sum()

var = [tas_timeseries_mean, sst_timeseries_mean]
names = ['tas', 'sst']

for i,n in zip(var, names):
    print(n)
    print(np.cov(i, rain_timeseries_mean))
    print()



"""# NOAA Dataset
- NDVI
"""

pip install s3fs

# pip install cartopy

# pip install boto3

# # imports
# import s3fs
# import xarray as xr
# import matplotlib.pyplot as plt
# import cartopy
# import cartopy.crs as ccrs
# import boto3
# import botocore
# import pooch
# import os
# import tempfile

# # connect to the AWS S3 bucket for the GPCP Monthly Precipitation CDR data
# fs = s3fs.S3FileSystem(anon=True)

# # get the list of all data files in the AWS S3 bucket fit the data file pattern.
# file_pattern = "noaa-cdr-ndvi-pds/data/*/AVHRR-Land_v005_AVH13C1_NOAA-*_*01_*.nc" # this only collects up to 2013
# file_location = fs.glob(file_pattern)

# file_location

# filelocation="http://s3.amazonaws.com/" + file_location[0]
# filelocation

# filename=file_location[0]
# filename

# # first, open a client connection
# client = boto3.client(
#     "s3", config=botocore.client.Config(signature_version=botocore.UNSIGNED)
# )  # initialize aws s3 bucket client

# # read single data file to understand the file structure
# ds_unique = xr.open_dataset(pooch.retrieve('http://s3.amazonaws.com/'+file_location[0],known_hash=None )) # open the file
# #ds_single = xr.open_dataset(
# #    pooch_load(
# #        filelocation="http://s3.amazonaws.com/" + file_location[0],
# #        filename=file_location[0],
# #    )
# #)
# # check how many variables are inluded in one data file
# ds_unique.data_vars
# ds_unique.coords

# # open all the monthly data files for all timespans
# # this process will take ~ 5 minute to complete due to the number of data files.

# file_ob = [pooch.retrieve('http://s3.amazonaws.com/'+file,known_hash=None ) for file in file_location]
# #file_ob = [
# #    pooch_load(filelocation="http://s3.amazonaws.com/" + file, filename=file)
# #    for file in file_location
# #]

# # connect to the AWS S3 bucket
# fs = s3fs.S3FileSystem(anon=True)

# # get the list of all data files in the AWS S3 bucket fit the data file pattern.
# file_pattern2 = "noaa-cdr-ndvi-pds/data/*/VIIRS-Land_v001-preliminary_NPP13C1_S-NPP_*01_*.nc" # this collects from 2014 onwards
# file_location2 = fs.glob(file_pattern2)

# filelocation2="http://s3.amazonaws.com/" + file_location2[0]
# filelocation2

# filename2=file_location2[0]
# filename2

# # first, open a client connection
# client = boto3.client(
#     "s3", config=botocore.client.Config(signature_version=botocore.UNSIGNED)
# )  # initialize aws s3 bucket client

# # read single data file to understand the file structure
# ds_unique2 = xr.open_dataset(pooch.retrieve('http://s3.amazonaws.com/'+file_location2[0],known_hash=None )) # open the file

# # open all the monthly data files for all timespans
# # this process will take ~ 5 minute to complete due to the number of data files.

# file_ob2 = [pooch.retrieve('http://s3.amazonaws.com/'+file,known_hash=None ) for file in file_location2]
# #file_ob = [
# #    pooch_load(filelocation="http://s3.amazonaws.com/" + file, filename=file)
# #    for file in file_location
# #]

# # using this function instead of 'open_dataset' will concatenate the data along the dimension we specify
# ds = xr.open_mfdataset((file_ob,file_ob2) combine="nested", concat_dim="time")

# # comment for colab users only: this could toss an error message for you.
# # you should still be able to use the dataset with this error just not print ds
# # you can try uncommenting the following line to avoid the error
# # ds.attrs['history']='' # the history attribute have unique chars that cause a crash on Google colab.
# ds.attrs

# ds.NDVI



"""# CMIP6 Dataset"""

# Commented out IPython magic to ensure Python compatibility.
# functions

# %matplotlib inline

col = intake.open_esm_datastore(
    "https://storage.googleapis.com/cmip6/pangeo-cmip6.json"
)  # open an intake catalog containing the Pangeo CMIP cloud data


def load_cmip6(source_id, variable_id, member_id, table_id):  # load selected model
    cat = col.search(
        source_id=source_ids,
        variable_id=variable_id,
        member_id=member_id,
        table_id=table_id,
        grid_label="gn",
        experiment_id=[
            "historical",
            "ssp126",
            "ssp245",
            "ssp585",
        ],  # downloading the scenarios out of the total 5+historical
        require_all_on=["source_id"],
    )

    kwargs = dict(
        preprocess=combined_preprocessing,
        xarray_open_kwargs=dict(use_cftime=True),
        storage_options={"token": "anon"},
    )
    cat.esmcat.aggregation_control.groupby_attrs = ["source_id", "experiment_id"]
    dt = cat.to_datatree(**kwargs)

    return dt

cat = col.search(
    source_id="GFDL-ESM4",
    variable_id=[
        "gpp",
        "npp",
        "nbp",
        "treeFrac",
        "grassFrac",
        "cropFrac",
        "pastureFrac",
    ],  # No 'shrubFrac','baresoilFrac','residualFrac' in GFDL-ESM4
    member_id="r1i1p1f1",
    table_id="Lmon", #["Oclim", "Amon", "Omon", "Lmon", "Limon", "Oimon", "Aero", "cfMon"]
    grid_label="gr1",
    experiment_id=["historical"],
    require_all_on=[
        "source_id"
    ],  # make sure that we only get models which have all of the above experiments
)

# convert the sub-catalog into a datatree object, by opening each dataset into an xarray.Dataset (without loading the data)
kwargs = dict(
    preprocess=combined_preprocessing,  # apply xMIP fixes to each dataset
    xarray_open_kwargs=dict(
        use_cftime=True
    ),  # ensure all datasets use the same time index
    storage_options={
        "token": "anon"
    },  # anonymous/public authentication to google cloud storage
)

cat.esmcat.aggregation_control.groupby_attrs = ["source_id", "experiment_id"]
dt_Lmon_variables = cat.to_datatree(**kwargs)

# convert to dataset instead of datatree, remove extra singleton dimensions
ds_Lmon = dt_Lmon_variables["GFDL-ESM4"]["historical"].to_dataset().squeeze()

# get monthly 'extension' variables

# from the full `col` object, create a subset using facet search
cat = col.search(
    source_id="GFDL-ESM4",
    variable_id="nep",
    member_id="r1i1p1f1",
    table_id="Emon",
    grid_label="gr1",
    experiment_id=["historical"],
    require_all_on=[
        "source_id"
    ],  # make sure that we only get models which have all of the above experiments
)

# convert the sub-catalog into a datatree object, by opening each dataset into an xarray.Dataset (without loading the data)
kwargs = dict(
    preprocess=combined_preprocessing,  # apply xMIP fixes to each dataset
    xarray_open_kwargs=dict(
        use_cftime=True
    ),  # ensure all datasets use the same time index
    storage_options={
        "token": "anon"
    },  # anonymous/public authentication to google cloud storage
)

cat.esmcat.aggregation_control.groupby_attrs = ["source_id", "experiment_id"]
dt_Emon_variables = cat.to_datatree(**kwargs)

# convert to dataset instead of datatree, remove extra singleton dimensions
ds_Emon = dt_Emon_variables["GFDL-ESM4"]["historical"].to_dataset().squeeze()

# get atmospheric variables

# from the full `col` object, create a subset using facet search
cat = col.search(
    source_id="GFDL-ESM4",
    variable_id=["rsds", "rsus", "tas", "pr"],
    member_id="r1i1p1f1",
    table_id="Amon",
    grid_label="gr1",
    experiment_id=["historical"],
    require_all_on=[
        "source_id"
    ],  # make sure that we only get models which have all of the above experiments
)

# convert the sub-catalog into a datatree object, by opening each dataset into an xarray.Dataset (without loading the data)
kwargs = dict(
    preprocess=combined_preprocessing,  # apply xMIP fixes to each dataset
    xarray_open_kwargs=dict(
        use_cftime=True
    ),  # ensure all datasets use the same time index
    storage_options={
        "token": "anon"
    },  # anonymous/public authentication to google cloud storage
)

cat.esmcat.aggregation_control.groupby_attrs = ["source_id", "experiment_id"]
dt_Amon_variables = cat.to_datatree(**kwargs)

# convert to dataset instead of datatree, remove extra singleton dimensions
ds_Amon = dt_Amon_variables["GFDL-ESM4"]["historical"].to_dataset().squeeze()

# get atmospheric variables

# from the full `col` object, create a subset using facet search
cat = col.search(
    source_id="GFDL-ESM4",
    variable_id=["areacella"],
    member_id="r1i1p1f1",
    table_id="fx",
    grid_label="gr1",
    experiment_id=["historical"],
    require_all_on=[
        "source_id"
    ],  # make sure that we only get models which have all of the above experiments
)

# convert the sub-catalog into a datatree object, by opening each dataset into an xarray.Dataset (without loading the data)
kwargs = dict(
    preprocess=combined_preprocessing,  # apply xMIP fixes to each dataset
    xarray_open_kwargs=dict(
        use_cftime=True
    ),  # ensure all datasets use the same time index
    storage_options={
        "token": "anon"
    },  # anonymous/public authentication to google cloud storage
)

cat.esmcat.aggregation_control.groupby_attrs = ["source_id", "experiment_id"]
dt_fx_variables = cat.to_datatree(**kwargs)

# convert to dataset instead of datatree, remove extra singleton dimensions
ds_fx = dt_fx_variables["GFDL-ESM4"]["historical"].to_dataset().squeeze()

# merge into single dataset. note, these are all on the 'gr1' grid.
ds = xr.Dataset()

# add land variables
for var in ds_Lmon.data_vars:
    ds[var] = ds_Lmon[var]

# add extension variables
for var in ds_Emon.data_vars:
    ds[var] = ds_Emon[var]

# add atmopsheric variables
for var in ds_Amon.data_vars:
    ds[var] = ds_Amon[var]

# add grid cell area
for var in ds_fx.data_vars:
    ds[var] = ds_fx[var]

# drop unnecessary coordinates
ds = ds.drop_vars(["member_id", "dcpp_init_year", "height"])
ds

## Variables meaning

# import pandas as pd
# from IPython.display import display, HTML, Markdown

# # Data as list of dictionaries
# classification_system = [
#     {
#         "Name": "gpp",
#         "Description": "Carbon Mass Flux out of Atmosphere due to Gross Primary Production on Land",
#     },
#     {
#         "Name": "npp",
#         "Description": "Carbon Mass Flux out of Atmosphere due to Net Primary Production on Land",
#     },
#     {
#         "Name": "nep",
#         "Description": "Carbon Mass Flux out of Atmophere due to Net Ecosystem Production on Land",
#     },
#     {
#         "Name": "nbp",
#         "Description": "Carbon Mass Flux out of Atmosphere due to Net Biospheric Production on Land",
#     },
#     {"Name": "treeFrac", "Description": "Land Area Percentage Tree Cover"},
#     {"Name": "grassFrac", "Description": "Land Area Percentage Natural Grass"},
#     {"Name": "cropFrac", "Description": "Land Area Percentage Crop Cover"},
#     {
#         "Name": "pastureFrac",
#         "Description": "Land Area Percentage Anthropogenic Pasture Cover",
#     },
#     {"Name": "rsus", "Description": "Surface Upwelling Shortwave Radiation"},
#     {"Name": "rsds", "Description": "Surface Downwelling Shortwave Radiation"},
#     {"Name": "tas", "Description": "Near-Surface Air Temperature"},
#     {"Name": "pr", "Description": "Precipitation"},
#     {
#         "Name": "areacella",
#         "Description": "Grid-Cell Area for Atmospheric Variables (all variabeles are on this grid however)",
#     },
# ]

# # df = pd.DataFrame(classification_system)
# # pd.set_option("display.max_colwidth", None)
# # html = df.to_html(index=False)
# # title_md = "### Table 1: CMIP6 Variables"
# # display(Markdown(title_md))
# # display(HTML(html))

## Coordinates Indonesia

x_ind = slice(95, 141) # longitude
y_ind = slice(-12,7) # latitude

print(ds.cropFrac.sel(time=slice('1850-01-01','2023-07-01'), y=y_ind, x=x_ind))

crop_frac = ds.cropFrac.sel(time=slice('1920-07-01','2023-07-01'), y=y_ind, x=x_ind)

1920+94/2

crop_ind_last = crop_frac[-1,:,:].squeeze()
crop_ind_mid = crop_frac[212,:,:].squeeze()
crop_ind_first = crop_frac[0,:,:].squeeze()

"""# Maps"""

fig, ax = plt.subplots(
    ncols=1, nrows=3, figsize=[15, 10], subplot_kw={"projection": ccrs.PlateCarree()}
)
# plot the model data
crop_ind_first.plot(
    ax=ax[0],
    x="lon",
    y="lat",
    vmin=crop_ind_first.min().compute(),
    vmax=crop_ind_first.max().compute(),
    cmap="viridis",
    robust=True # This deletes outliers: use 2nd and 98th percentiles of data
)

crop_ind_mid.plot(
    ax=ax[1],
    x="lon",
    y="lat",
    vmin=crop_ind_mid.min().compute(),
    vmax=crop_ind_mid.max().compute(),
    cmap="viridis",
    robust=True # This deletes outliers: use 2nd and 98th percentiles of data
)

crop_ind_last.plot(
    ax=ax[2],
    x="lon",
    y="lat",
    vmin=crop_ind_last.min().compute(),
    vmax=crop_ind_last.max().compute(),
    cmap="viridis",
    robust=True
)
ax[0].coastlines()
#ax[0].set_title(f"Indonesia Tree Cover - {np.datetime_as_string(ts_ind_first['time'].values, unit='D')}")

ax[1].coastlines()
#ax[1].set_title(f"Indonesia Tree Cover - {np.datetime_as_string(ts_ind_last['time'].values, unit='D')}")

ax[2].coastlines()
#ax[1].set_title(f"Indonesia Tree Cover - {np.datetime_as_string(ts_ind_last['time'].values, unit='D')}")

crop_frac = ds.treeFrac.sel(time=slice('1920-07-01','2023-07-01'), y=y_ind, x=x_ind)
crop_ind_last = crop_frac[-1,:,:].squeeze()
crop_ind_mid = crop_frac[212,:,:].squeeze()
crop_ind_first = crop_frac[0,:,:].squeeze()

fig, ax = plt.subplots(
    ncols=1, nrows=3, figsize=[15, 10], subplot_kw={"projection": ccrs.PlateCarree()}
)
# plot the model data
crop_ind_first.plot(
    ax=ax[0],
    x="lon",
    y="lat",
    vmin=crop_ind_first.min().compute(),
    vmax=crop_ind_first.max().compute(),
    cmap="viridis",
    robust=True # This deletes outliers: use 2nd and 98th percentiles of data
)

crop_ind_mid.plot(
    ax=ax[1],
    x="lon",
    y="lat",
    vmin=crop_ind_mid.min().compute(),
    vmax=crop_ind_mid.max().compute(),
    cmap="viridis",
    robust=True # This deletes outliers: use 2nd and 98th percentiles of data
)

crop_ind_last.plot(
    ax=ax[2],
    x="lon",
    y="lat",
    vmin=crop_ind_last.min().compute(),
    vmax=crop_ind_last.max().compute(),
    cmap="viridis",
    robust=True
)
ax[0].coastlines()
#ax[0].set_title(f"Indonesia Tree Cover - {np.datetime_as_string(ts_ind_first['time'].values, unit='D')}")

ax[1].coastlines()
#ax[1].set_title(f"Indonesia Tree Cover - {np.datetime_as_string(ts_ind_last['time'].values, unit='D')}")

ax[2].coastlines()
#ax[1].set_title(f"Indonesia Tree Cover - {np.datetime_as_string(ts_ind_last['time'].values, unit='D')}")

crop_frac = ds.grassFrac.sel(time=slice('1920-07-01','2023-07-01'), y=y_ind, x=x_ind)
crop_ind_last = crop_frac[-1,:,:].squeeze()
crop_ind_mid = crop_frac[212,:,:].squeeze()
crop_ind_first = crop_frac[0,:,:].squeeze()

fig, ax = plt.subplots(
    ncols=1, nrows=3, figsize=[15, 10], subplot_kw={"projection": ccrs.PlateCarree()}
)
# plot the model data
crop_ind_first.plot(
    ax=ax[0],
    x="lon",
    y="lat",
    vmin=crop_ind_first.min().compute(),
    vmax=crop_ind_first.max().compute(),
    cmap="viridis",
    robust=True # This deletes outliers: use 2nd and 98th percentiles of data
)

crop_ind_mid.plot(
    ax=ax[1],
    x="lon",
    y="lat",
    vmin=crop_ind_mid.min().compute(),
    vmax=crop_ind_mid.max().compute(),
    cmap="viridis",
    robust=True # This deletes outliers: use 2nd and 98th percentiles of data
)

crop_ind_last.plot(
    ax=ax[2],
    x="lon",
    y="lat",
    vmin=crop_ind_last.min().compute(),
    vmax=crop_ind_last.max().compute(),
    cmap="viridis",
    robust=True
)
ax[0].coastlines()
#ax[0].set_title(f"Indonesia Tree Cover - {np.datetime_as_string(ts_ind_first['time'].values, unit='D')}")

ax[1].coastlines()
#ax[1].set_title(f"Indonesia Tree Cover - {np.datetime_as_string(ts_ind_last['time'].values, unit='D')}")

ax[2].coastlines()
#ax[1].set_title(f"Indonesia Tree Cover - {np.datetime_as_string(ts_ind_last['time'].values, unit='D')}")

crop_frac = ds.pastureFrac.sel(time=slice('1920-07-01','2023-07-01'), y=y_ind, x=x_ind)
crop_ind_last = crop_frac[-1,:,:].squeeze()
crop_ind_mid = crop_frac[212,:,:].squeeze()
crop_ind_first = crop_frac[0,:,:].squeeze()

fig, ax = plt.subplots(
    ncols=1, nrows=3, figsize=[15, 10], subplot_kw={"projection": ccrs.PlateCarree()}
)
# plot the model data
crop_ind_first.plot(
    ax=ax[0],
    x="lon",
    y="lat",
    vmin=crop_ind_first.min().compute(),
    vmax=crop_ind_first.max().compute(),
    cmap="viridis",
    robust=True # This deletes outliers: use 2nd and 98th percentiles of data
)

crop_ind_mid.plot(
    ax=ax[1],
    x="lon",
    y="lat",
    vmin=crop_ind_mid.min().compute(),
    vmax=crop_ind_mid.max().compute(),
    cmap="viridis",
    robust=True # This deletes outliers: use 2nd and 98th percentiles of data
)

crop_ind_last.plot(
    ax=ax[2],
    x="lon",
    y="lat",
    vmin=crop_ind_last.min().compute(),
    vmax=crop_ind_last.max().compute(),
    cmap="viridis",
    robust=True
)
ax[0].coastlines()
#ax[0].set_title(f"Indonesia Tree Cover - {np.datetime_as_string(ts_ind_first['time'].values, unit='D')}")

ax[1].coastlines()
#ax[1].set_title(f"Indonesia Tree Cover - {np.datetime_as_string(ts_ind_last['time'].values, unit='D')}")

ax[2].coastlines()
#ax[1].set_title(f"Indonesia Tree Cover - {np.datetime_as_string(ts_ind_last['time'].values, unit='D')}")

crop_frac = ds.pr.sel(time=slice('1920-07-01','2023-07-01'), y=y_ind, x=x_ind)
crop_ind_last = crop_frac[-1,:,:].squeeze()
crop_ind_mid = crop_frac[212,:,:].squeeze()
crop_ind_first = crop_frac[0,:,:].squeeze()

fig, ax = plt.subplots(
    ncols=1, nrows=3, figsize=[15, 10], subplot_kw={"projection": ccrs.PlateCarree()}
)
# plot the model data
crop_ind_first.plot(
    ax=ax[0],
    x="lon",
    y="lat",
    vmin=crop_ind_first.min().compute(),
    vmax=crop_ind_first.max().compute(),
    cmap="viridis",
    robust=True # This deletes outliers: use 2nd and 98th percentiles of data
)

crop_ind_mid.plot(
    ax=ax[1],
    x="lon",
    y="lat",
    vmin=crop_ind_mid.min().compute(),
    vmax=crop_ind_mid.max().compute(),
    cmap="viridis",
    robust=True # This deletes outliers: use 2nd and 98th percentiles of data
)

crop_ind_last.plot(
    ax=ax[2],
    x="lon",
    y="lat",
    vmin=crop_ind_last.min().compute(),
    vmax=crop_ind_last.max().compute(),
    cmap="viridis",
    robust=True
)
ax[0].coastlines()
#ax[0].set_title(f"Indonesia Tree Cover - {np.datetime_as_string(ts_ind_first['time'].values, unit='D')}")

ax[1].coastlines()
#ax[1].set_title(f"Indonesia Tree Cover - {np.datetime_as_string(ts_ind_last['time'].values, unit='D')}")

ax[2].coastlines()
#ax[1].set_title(f"Indonesia Tree Cover - {np.datetime_as_string(ts_ind_last['time'].values, unit='D')}")



crop_frac = ds.rsds.sel(time=slice('1920-07-01','2023-07-01'), y=y_ind, x=x_ind)
crop_ind_last = crop_frac[-1,:,:].squeeze()
crop_ind_mid = crop_frac[212,:,:].squeeze()
crop_ind_first = crop_frac[0,:,:].squeeze()

fig, ax = plt.subplots(
    ncols=1, nrows=3, figsize=[15, 10], subplot_kw={"projection": ccrs.PlateCarree()}
)
# plot the model data
crop_ind_first.plot(
    ax=ax[0],
    x="lon",
    y="lat",
    vmin=crop_ind_first.min().compute(),
    vmax=crop_ind_first.max().compute(),
    cmap="viridis",
    robust=True # This deletes outliers: use 2nd and 98th percentiles of data
)

crop_ind_mid.plot(
    ax=ax[1],
    x="lon",
    y="lat",
    vmin=crop_ind_mid.min().compute(),
    vmax=crop_ind_mid.max().compute(),
    cmap="viridis",
    robust=True # This deletes outliers: use 2nd and 98th percentiles of data
)

crop_ind_last.plot(
    ax=ax[2],
    x="lon",
    y="lat",
    vmin=crop_ind_last.min().compute(),
    vmax=crop_ind_last.max().compute(),
    cmap="viridis",
    robust=True
)
ax[0].coastlines()
#ax[0].set_title(f"Indonesia Tree Cover - {np.datetime_as_string(ts_ind_first['time'].values, unit='D')}")

ax[1].coastlines()
#ax[1].set_title(f"Indonesia Tree Cover - {np.datetime_as_string(ts_ind_last['time'].values, unit='D')}")

ax[2].coastlines()
#ax[1].set_title(f"Indonesia Tree Cover - {np.datetime_as_string(ts_ind_last['time'].values, unit='D')}")

crop_frac = ds.rsus.sel(time=slice('1920-07-01','2023-07-01'), y=y_ind, x=x_ind)
crop_ind_last = crop_frac[-1,:,:].squeeze()
crop_ind_mid = crop_frac[212,:,:].squeeze()
crop_ind_first = crop_frac[0,:,:].squeeze()

fig, ax = plt.subplots(
    ncols=1, nrows=3, figsize=[15, 10], subplot_kw={"projection": ccrs.PlateCarree()}
)
# plot the model data
crop_ind_first.plot(
    ax=ax[0],
    x="lon",
    y="lat",
    vmin=crop_ind_first.min().compute(),
    vmax=crop_ind_first.max().compute(),
    cmap="viridis",
    robust=True # This deletes outliers: use 2nd and 98th percentiles of data
)

crop_ind_mid.plot(
    ax=ax[1],
    x="lon",
    y="lat",
    vmin=crop_ind_mid.min().compute(),
    vmax=crop_ind_mid.max().compute(),
    cmap="viridis",
    robust=True # This deletes outliers: use 2nd and 98th percentiles of data
)

crop_ind_last.plot(
    ax=ax[2],
    x="lon",
    y="lat",
    vmin=crop_ind_last.min().compute(),
    vmax=crop_ind_last.max().compute(),
    cmap="viridis",
    robust=True
)
ax[0].coastlines()
#ax[0].set_title(f"Indonesia Tree Cover - {np.datetime_as_string(ts_ind_first['time'].values, unit='D')}")

ax[1].coastlines()
#ax[1].set_title(f"Indonesia Tree Cover - {np.datetime_as_string(ts_ind_last['time'].values, unit='D')}")

ax[2].coastlines()
#ax[1].set_title(f"Indonesia Tree Cover - {np.datetime_as_string(ts_ind_last['time'].values, unit='D')}")

crop_frac = ds.tas.sel(time=slice('1920-07-01','2023-07-01'), y=y_ind, x=x_ind)
crop_ind_last = crop_frac[-1,:,:].squeeze()
crop_ind_mid = crop_frac[212,:,:].squeeze()
crop_ind_first = crop_frac[0,:,:].squeeze()

fig, ax = plt.subplots(
    ncols=1, nrows=3, figsize=[15, 10], subplot_kw={"projection": ccrs.PlateCarree()}
)
# plot the model data
crop_ind_first.plot(
    ax=ax[0],
    x="lon",
    y="lat",
    vmin=crop_ind_first.min().compute(),
    vmax=crop_ind_first.max().compute(),
    cmap="viridis",
    robust=True # This deletes outliers: use 2nd and 98th percentiles of data
)

crop_ind_mid.plot(
    ax=ax[1],
    x="lon",
    y="lat",
    vmin=crop_ind_mid.min().compute(),
    vmax=crop_ind_mid.max().compute(),
    cmap="viridis",
    robust=True # This deletes outliers: use 2nd and 98th percentiles of data
)

crop_ind_last.plot(
    ax=ax[2],
    x="lon",
    y="lat",
    vmin=crop_ind_last.min().compute(),
    vmax=crop_ind_last.max().compute(),
    cmap="viridis",
    robust=True
)
ax[0].coastlines()
#ax[0].set_title(f"Indonesia Tree Cover - {np.datetime_as_string(ts_ind_first['time'].values, unit='D')}")

ax[1].coastlines()
#ax[1].set_title(f"Indonesia Tree Cover - {np.datetime_as_string(ts_ind_last['time'].values, unit='D')}")

ax[2].coastlines()
#ax[1].set_title(f"Indonesia Tree Cover - {np.datetime_as_string(ts_ind_last['time'].values, unit='D')}")

## all variables for Indonesia

Indonesia = ds.sel(y=y_ind, x=x_ind)
#Indonesia

type(Indonesia)

#df_Indonesia = Indonesia.to_dataframe()

## Get the timeseries of the data so that I have the country mean of all variables along all time points
import pandas as pd

Indonesia_timeseries = ds.sel( y=y_ind, x=x_ind)
Indonesia_timeseries = Indonesia_timeseries.mean(['y','x'], keep_attrs=True)
type(Indonesia_timeseries)
Indonesia_timeseries_mean = Indonesia_timeseries.to_dataframe()
Indonesia_timeseries_mean.head(5)

Indonesia_timeseries_mean.shape

Indonesia_timeseries_mean.to_csv('Indonesia_timeseries_mean.csv', index=True)

"""# Covariance and correlation analysis"""

# crops
# crop_frac
# rain
# rain = do_pr.precip.sel(time=slice('1979-01-01','2014-12-16'), lat=y_ind, lon=x_ind)
# sst
# sst = do_sst.sst.sel(time=slice('1979-01-01','2023-07-01'), lat=y_ind, lon=x_ind)
# air temp anom
# tas = do_tas.air.sel(time=slice('1979-01-01','2023-07-01'), lat=y_ind, lon=x_ind)

# take the timeseries
## Coordinates Indonesia

x_ind = slice(95, 141) # longitude
y_ind = slice(7,-12) # latitude

## SST

sst_timeseries = do_sst.sel(time=slice('1979-07-01','2014-12-01'), lat=y_ind, lon=x_ind)
sst_timeseries = sst_timeseries.mean(["lon", "lat"], keep_attrs=True)
sst_timeseries_mean = np.array(sst_timeseries.sst)

## Air temp anomalies

tas_timeseries = do_tas.sel(time=slice('1979-07-01','2014-12-01'), lat=y_ind, lon=x_ind)
tas_timeseries = tas_timeseries.mean(["lon", "lat"], keep_attrs=True)
tas_timeseries_mean = np.array(tas_timeseries.air)

## Precipitation

rain_timeseries = do_pr.sel(time=slice('1979-07-01','2014-12-01'), lat=y_ind, lon=x_ind)
rain_timeseries = rain_timeseries.mean(["lon", "lat"], keep_attrs=True)
rain_timeseries_mean = np.array(rain_timeseries.precip)

## Crop fraction

x_ind = slice(95, 141) # longitude
y_ind = slice(-12,7) # latitude

crop_frac_timeseries = ds.sel(time=slice('1979-07-01','2014-12-16'), y=y_ind, x=x_ind)
crop_frac_timeseries = crop_frac_timeseries.mean(['y','x'], keep_attrs=True)
crop_frac_timeseries_mean = np.array(crop_frac_timeseries.cropFrac)

var = [rain_timeseries_mean, tas_timeseries_mean, sst_timeseries_mean]

print(len(tas_timeseries_mean))
print(len(crop_frac_timeseries_mean))
print(len(rain_timeseries_mean))
print(len(sst_timeseries_mean))

## Plots


tas_timeseries.air.plot()
plt.show();
print()

rain_timeseries.precip.plot()
plt.show();
print()

sst_timeseries.sst.plot()
plt.show();
print()

crop_frac_timeseries.cropFrac.plot()
plt.show();

from statsmodels.tsa.seasonal import seasonal_decompose

# Decomposition

result = seasonal_decompose(tas_timeseries_mean, model='additive', period=12)
result.plot();

result = seasonal_decompose(rain_timeseries_mean, model='additive', period=12)
result.plot();

result = seasonal_decompose(sst_timeseries_mean, model='additive', period=12)
result.plot();

result = seasonal_decompose(crop_frac_timeseries_mean, model='additive', period=12)
result.plot();

names = ['rain','tas', 'sst']

## Covariance

for i,n in zip(var, names):
    print(n)
    print(np.cov(i, crop_frac_timeseries_mean))
    print()

## Correlation

# Pearson for linear correlation

from scipy.stats import pearsonr

for i,n in zip(var, names):
    print(n)
    print(round(pearsonr(i, crop_frac_timeseries_mean)[1],5))
    print()

# Spearman for nonlinear correlation
# If  you are unsure of the distribution and possible relationships between two variables, Spearman correlation coefficient is a good tool to use.
from scipy.stats import spearmanr

for i,n in zip(var, names):
    print(n)
    print(round(spearmanr(i, crop_frac_timeseries_mean)[1],5))
    print()

"""# Stationarity test"""

# print(len(tas_timeseries_mean))
# print(len(crop_frac_timeseries_mean))
# print(len(rain_timeseries_mean))
# print(len(sst_timeseries_mean))

plt.hist(tas_timeseries_mean);
stats.describe(tas_timeseries_mean)

plt.hist(crop_frac_timeseries_mean);
stats.describe(crop_frac_timeseries_mean)

plt.hist(rain_timeseries_mean);
stats.describe(rain_timeseries_mean)

plt.hist(sst_timeseries_mean);
stats.describe(sst_timeseries_mean)

values = [sst_timeseries_mean, rain_timeseries_mean, tas_timeseries_mean, crop_frac_timeseries_mean]
names = ['sst', 'rain', 'tas', 'crop']

## ADF

from statsmodels.tsa.stattools import adfuller

for value, key in zip(values, names):
  print(key)
  result = adfuller(value)
  print('ADF Statistic: %f' % result[0])
  print('p-value: %f' % result[1])
  print('Critical Values:')
  for key, value in result[4].items():
    print('\t%s: %.3f' % (key, value))

from statsmodels.tsa.stattools import kpss
import warnings
warnings.filterwarnings("ignore")


for value, key in zip(values, names):
  print(key)
  result = kpss(value)
  print('KPSS Statistic: %f' % result[0])
  print('p-value: %f' % result[1])
  print()

## It looks like our crop variable is trend stationary

# pip install arch

import arch
from arch.unitroot import PhillipsPerron as pp

for value, key in zip(values, names):
  print(key)
  result = pp(value)
  print('PP Statistic: %f' % result.stat)
  print('p-value: %f' % result.pvalue)
  print()

for value, key in zip(values, names):
  print(key)
  result = pp(value, trend='ct')
  print('PP Statistic: %f' % result.stat)
  print('p-value: %f' % result.pvalue)
  print()

"""### Stationarity test with first difference"""

# Take the first difference

## Plots


plt.plot(np.diff(tas_timeseries.air))
plt.show();
print()


plt.plot(np.diff(rain_timeseries.precip))
plt.show();
print()

plt.plot(np.diff(sst_timeseries.sst))
plt.show();
print()

plt.plot(np.diff(crop_frac_timeseries.cropFrac))
plt.show();

values = [np.diff(sst_timeseries_mean), np.diff(rain_timeseries_mean), np.diff(tas_timeseries_mean), np.diff(crop_frac_timeseries_mean)]
names = ['sst', 'rain', 'tas', 'crop']

## ADF

for value, key in zip(values, names):
  print(key)
  result = adfuller(value)
  print('ADF Statistic: %f' % result[0])
  print('p-value: %f' % result[1])
  print('Critical Values:')
  for key, value in result[4].items():
    print('\t%s: %.3f' % (key, value))

for value, key in zip(values, names):
  print(key)
  result = kpss(value)
  print('KPSS Statistic: %f' % result[0])
  print('p-value: %f' % result[1])
  print()

for value, key in zip(values, names):
  print(key)
  result = pp(value)
  print('PP Statistic: %f' % result.stat)
  print('p-value: %f' % result.pvalue)
  print()

for value, key in zip(values, names):
  print(key)
  result = pp(value, trend='ct')
  print('PP Statistic: %f' % result.stat)
  print('p-value: %f' % result.pvalue)
  print()

## Staionarity test considering all years of the time series

# take the timeseries
## Coordinates Indonesia

x_ind = slice(95, 141) # longitude
y_ind = slice(7,-12) # latitude

## SST

sst_timeseries = do_sst.sel(lat=y_ind, lon=x_ind)
sst_timeseries = sst_timeseries.mean(["lon", "lat"], keep_attrs=True)
sst_timeseries_mean = np.array(sst_timeseries.sst)

## Air temp anomalies

tas_timeseries = do_tas.sel(lat=y_ind, lon=x_ind)
tas_timeseries = tas_timeseries.mean(["lon", "lat"], keep_attrs=True)
tas_timeseries_mean = np.array(tas_timeseries.air)

## Precipitation

rain_timeseries = do_pr.sel(lat=y_ind, lon=x_ind)
rain_timeseries = rain_timeseries.mean(["lon", "lat"], keep_attrs=True)
rain_timeseries_mean = np.array(rain_timeseries.precip)

values = [sst_timeseries_mean, rain_timeseries_mean, tas_timeseries_mean, crop_frac_timeseries_mean]
names = ['sst', 'rain', 'tas', 'crop']

## ADF

from statsmodels.tsa.stattools import adfuller

for value, key in zip(values, names):
  print(key)
  result = adfuller(value)
  print('ADF Statistic: %f' % result[0])
  print('p-value: %f' % result[1])
  print('Critical Values:')
  for key, value in result[4].items():
    print('\t%s: %.3f' % (key, value))

from statsmodels.tsa.stattools import kpss
import warnings
warnings.filterwarnings("ignore")


for value, key in zip(values, names):
  print(key)
  result = kpss(value)
  print('KPSS Statistic: %f' % result[0])
  print('p-value: %f' % result[1])
  print()

## Nothing really changed

# Let' s try detrending the crop variable and see if the results change
from scipy.signal import detrend
import pandas as pd

detrended = detrend(crop_frac_timeseries_mean, type='linear')
detrended = pd.Series(detrended)

## ADF


result = adfuller(detrended)
print('ADF Statistic: %f' % result[0])
print('p-value: %f' % result[1])
print(f'Critical Values: {result}')

"""# Plot CMIP6 data"""

